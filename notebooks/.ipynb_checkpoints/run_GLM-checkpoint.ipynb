{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f698b3-c8f4-4f93-b8e1-c074b2d7aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pluta lab\\.conda\\envs\\neural_analysis\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Check which env you are in: Should be 'C:\\Users\\Pluta lab\\.conda\\envs\\neural_analysis\\python.exe'\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573a3f84-99b0-4421-9f51-e9927babdb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6335aa1c-a01b-407a-bb38-da544e6c2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# # Add parent directory to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "parent_dir = parent_dir + '\\\\'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.data_processing import shuffle_spike_data\n",
    "from utils.feature_engineering import create_lagged_features, standardize_features\n",
    "from utils.model_evaluation import fit_and_evaluate\n",
    "from utils.model_selection import find_best_alpha\n",
    "from config.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9679658c-867f-43cb-8508-888a55ed4da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing neuron 1/82 for E40\n",
      "Processing neuron 2/82 for E40\n",
      "Processing neuron 3/82 for E40\n",
      "Processing neuron 4/82 for E40\n",
      "Processing neuron 5/82 for E40\n",
      "Processing neuron 6/82 for E40\n",
      "Processing neuron 7/82 for E40\n",
      "Processing neuron 8/82 for E40\n",
      "Processing neuron 9/82 for E40\n",
      "Processing neuron 10/82 for E40\n",
      "Model results saved to D:\\JUPYTER Analysis\\neural_temporal_GLM\\results\\model_results\\E40_modelresults.pkl\n",
      "Processing neuron 1/69 for E45\n",
      "Processing neuron 2/69 for E45\n",
      "Processing neuron 3/69 for E45\n",
      "Processing neuron 4/69 for E45\n",
      "Processing neuron 5/69 for E45\n",
      "Processing neuron 6/69 for E45\n",
      "Processing neuron 7/69 for E45\n",
      "Processing neuron 8/69 for E45\n",
      "Processing neuron 9/69 for E45\n",
      "Processing neuron 10/69 for E45\n",
      "Model results saved to D:\\JUPYTER Analysis\\neural_temporal_GLM\\results\\model_results\\E45_modelresults.pkl\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file_paths = [\n",
    "    \"data/raw/neural_data_E40.mat\",\n",
    "    \"data/raw/neural_data_E45.mat\"\n",
    "]\n",
    "\n",
    "# Create output directory\n",
    "results_dir = parent_dir + \"results\"\n",
    "output_dir = os.path.join(results_dir, \"model_results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Main analysis loop\n",
    "for file_path in file_paths:\n",
    "    # Load the .mat file\n",
    "    data = scipy.io.loadmat(parent_dir + file_path)\n",
    "\n",
    "    # Extract experiment identifier from the last 3 characters of the file name\n",
    "    exp_identifier = re.search(r'E\\d{2,3}', file_path).group(0)\n",
    "\n",
    "    # Extract variables\n",
    "    spike_data = data['spike_fw_raw']\n",
    "    run_data = data['run_fw_raw']\n",
    "    stpt_data = data['stpt_fw_raw']\n",
    "    amp_data = data['amp_fw_raw']\n",
    "    select_time_bins = data['select_time_bins'].flatten()\n",
    "\n",
    "    # Initialize result variables\n",
    "    r2 = []\n",
    "    mse = []\n",
    "    r2_shuffled = []\n",
    "    coefficients_dict = {}\n",
    "    p_values = []\n",
    "    alpha = []\n",
    "\n",
    "    # Loop over each neuron\n",
    "    for neuron in range(10):#spike_data.shape[1]):\n",
    "        print(f\"Processing neuron {neuron + 1}/{spike_data.shape[1]} for {exp_identifier}\")\n",
    "\n",
    "        # Extract data for the current neuron\n",
    "        neuron_spike_data = spike_data[:, neuron]\n",
    "\n",
    "        # Convert data to a DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'spike_data': neuron_spike_data,\n",
    "            'run_data': run_data.flatten(),\n",
    "            'stpt_data': stpt_data.flatten(),\n",
    "            'amp_data': amp_data.flatten(),\n",
    "            'time_bins': select_time_bins\n",
    "        })\n",
    "        df.sort_values(by='time_bins', inplace=True)\n",
    "\n",
    "        # Create lagged features\n",
    "        X, y = create_lagged_features(df, config.lags, df['time_bins'].values)\n",
    "\n",
    "        # Standardize features\n",
    "        X_scaled, scaler = standardize_features(X)\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Find the best alpha using grid search\n",
    "        best_alpha = find_best_alpha(X_train, y_train, config.alpha_values)\n",
    "        alpha.append(best_alpha)\n",
    "\n",
    "        # Fit the best model and evaluate\n",
    "        mse_value, r2_value, intercept, coefficients, y_pred = fit_and_evaluate(\n",
    "            X_train, y_train, X_test, y_test, best_alpha\n",
    "        )\n",
    "        mse.append(mse_value)\n",
    "        r2.append(r2_value)\n",
    "\n",
    "        # Permutation test\n",
    "        r2_shuffle_results = []\n",
    "        for _ in range(config.n_shuffles):\n",
    "            shuffled_y_train = shuffle_spike_data(y_train)\n",
    "            _, r2_shuffled_value, _, _, _ = fit_and_evaluate(\n",
    "                X_train, shuffled_y_train, X_test, y_test, best_alpha\n",
    "            )\n",
    "            r2_shuffle_results.append(r2_shuffled_value)\n",
    "\n",
    "        r2_shuffled.append(r2_shuffle_results)\n",
    "\n",
    "        # Reshape coefficients for saving\n",
    "        num_features = 3\n",
    "        coefficients_reshaped = coefficients.reshape((num_features, 2 * config.lags + 1))\n",
    "        coefficients_dict[neuron] = coefficients_reshaped\n",
    "\n",
    "        # Compute p-value\n",
    "        p_value = (np.sum(np.array(r2_shuffle_results) >= r2_value) + 1) / (len(r2_shuffle_results) + 1)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    # Save results to a pickle file\n",
    "    results_filename = os.path.join(output_dir, f\"{exp_identifier}_modelresults.pkl\")\n",
    "    model_results = {\n",
    "        'p_values': p_values,\n",
    "        'r2': r2,\n",
    "        'mse': mse,\n",
    "        'coefficients_dict': coefficients_dict,\n",
    "        'alpha': alpha,\n",
    "    }\n",
    "    with open(results_filename, 'wb') as f:\n",
    "        pickle.dump(model_results, f)\n",
    "\n",
    "    print(f\"Model results saved to {results_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61109a5-ba88-4682-9ec4-6876c4576b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neural_analysis)",
   "language": "python",
   "name": "neural_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
